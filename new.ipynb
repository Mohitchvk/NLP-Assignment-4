{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pcFMwQ03fN_"
   },
   "source": [
    "## CS 6120: Natural Language Processing - Prof. Ahmad Uzair\n",
    "\n",
    "### Assignment 4: EM, Heirarchical Clustering and HMM\n",
    "### Total Points: 100 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YRnxpPm3yus"
   },
   "source": [
    "In Assignment 4, you will implement Viterbi Algorithm based on Hidden Markov Model for Part-of-speech tagging. We recommend you to start this assignment a little early and fully understand these algorithms before jumping into coding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEAiEe-ZxW5r"
   },
   "source": [
    "## Parts of speech tagging. \n",
    "\n",
    "Parts of Speech Tagging is the process of the assigning a parts of speech tag (noun, adjective etc..,) to each word in the input sentence. <br>\n",
    "\n",
    "In this question we will be building HMMs and Viterbi Algorithm. \n",
    "\n",
    "### About the Dataset: <br>\n",
    "\n",
    "For this task we will be using tagged datasets collected from Wall Street Journal. <br>\n",
    "\n",
    "The file train.pos will be used for training and test.pos will for testing. Along with these two we will be providing vocab.txt the words in this file are the words from the training set that were used two or more times.<br>\n",
    "\n",
    "The dataset will contain different tags like JJ which means adjective, DT means determiner etc.., for better understaning of the tags refer to [this link](http://relearn.be/2015/training-common-sense/sources/software/pattern-2.6-critical-fork/docs/html/mbsp-tags.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rg5E-2k7xbP6"
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "##### Dont change anything in this code cell , only change the data paths accordingly  #######\n",
    "##############################################################################################\n",
    "\n",
    "#Importing necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "# Punctuation characters\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "# Morphology rules used to assign unknown word tokens\n",
    "noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
    "verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
    "adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
    "\n",
    "# Utility functions which we further need\n",
    "\n",
    "def get_word_tag(line, vocab): \n",
    "    if not line.split():\n",
    "        word = \"--n--\"\n",
    "        tag = \"--s--\"\n",
    "        return word, tag\n",
    "    else:\n",
    "        word, tag = line.split()\n",
    "        if word not in vocab: \n",
    "            # Handle unknown words\n",
    "            word = assign_unk(word)\n",
    "        return word, tag\n",
    "    return None \n",
    "\n",
    "\n",
    "def preprocess(vocab, data_fp):\n",
    "    \"\"\"\n",
    "    Preprocess data\n",
    "    \"\"\"\n",
    "    orig = []\n",
    "    prep = []\n",
    "\n",
    "    # Read data\n",
    "    with open(data_fp, \"r\") as data_file:\n",
    "\n",
    "        for cnt, word in enumerate(data_file):\n",
    "\n",
    "            # End of sentence\n",
    "            if not word.split():\n",
    "                orig.append(word.strip())\n",
    "                word = \"--n--\"\n",
    "                prep.append(word)\n",
    "                continue\n",
    "\n",
    "            # Handle unknown words\n",
    "            elif word.strip() not in vocab:\n",
    "                orig.append(word.strip())\n",
    "                word = assign_unk(word)\n",
    "                prep.append(word)\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                orig.append(word.strip())\n",
    "                prep.append(word.strip())\n",
    "\n",
    "    assert(len(orig) == len(open(data_fp, \"r\").readlines()))\n",
    "    assert(len(prep) == len(open(data_fp, \"r\").readlines()))\n",
    "\n",
    "    return orig, prep\n",
    "\n",
    "\n",
    "def assign_unk(tok):\n",
    "    \"\"\"\n",
    "    Assign unknown word tokens\n",
    "    \"\"\"\n",
    "    # Digits\n",
    "    if any(char.isdigit() for char in tok):\n",
    "        return \"--unk_digit--\"\n",
    "\n",
    "    # Punctuation\n",
    "    elif any(char in punct for char in tok):\n",
    "        return \"--unk_punct--\"\n",
    "\n",
    "    # Upper-case\n",
    "    elif any(char.isupper() for char in tok):\n",
    "        return \"--unk_upper--\"\n",
    "\n",
    "    # Nouns\n",
    "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"--unk_noun--\"\n",
    "\n",
    "    # Verbs\n",
    "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"--unk_verb--\"\n",
    "\n",
    "    # Adjectives\n",
    "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"--unk_adj--\"\n",
    "\n",
    "    # Adverbs\n",
    "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"--unk_adv--\"\n",
    "\n",
    "    return \"--unk--\"\n",
    "\n",
    "\n",
    "# for viterbi\n",
    "def compute_accuracy(pred, y):\n",
    "    '''\n",
    "    Input: \n",
    "        pred: a list of the predicted parts-of-speech \n",
    "        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\n",
    "    Output: \n",
    "        \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Zip together the prediction and the labels\n",
    "    for prediction, y in zip(pred, y):\n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        # Split the label into the word and the POS tag\n",
    "        word_tag_tuple = y.split()\n",
    "        \n",
    "        # Check that there is actually a word and a tag\n",
    "        # no more and no less than 2 items\n",
    "        if len(word_tag_tuple)!=2: # complete this line\n",
    "            continue \n",
    "\n",
    "        # store the word and tag separately\n",
    "        word, tag = word_tag_tuple\n",
    "        \n",
    "        # Check if the POS tag label matches the prediction\n",
    "        if prediction == tag: # complete this line\n",
    "            \n",
    "            # count the number of times that the prediction\n",
    "            # and label match\n",
    "            num_correct += 1\n",
    "            \n",
    "        # keep track of the total number of examples (that have valid labels)\n",
    "        total += 1\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return num_correct/total\n",
    "\n",
    "\n",
    "# load in the training corpus\n",
    "with open(\"training.pos.txt\", 'r') as f:\n",
    "    training_corpus = f.readlines()\n",
    "\n",
    "# read the vocabulary data, split by each line of text, and save the list\n",
    "with open(\"vocab.txt\", 'r') as f:\n",
    "    voc_l = f.read().split('\\n')\n",
    "\n",
    "# vocab: dictionary that has the index of the corresponding words\n",
    "vocab = {} \n",
    "\n",
    "# Get the index of the corresponding words. \n",
    "for i, word in enumerate(sorted(voc_l)): \n",
    "    vocab[word] = i       \n",
    "    \n",
    "\n",
    "cnt = 0\n",
    "for k,v in vocab.items():\n",
    "    cnt += 1\n",
    "    if cnt > 20:\n",
    "        break\n",
    "\n",
    "# load in the test corpus\n",
    "with open(\"test.pos.txt\", 'r') as f:\n",
    "    y = f.readlines()\n",
    "\n",
    "#corpus without tags, preprocessed\n",
    "_, prep = preprocess(vocab, \"test.words.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiF3lMICSIVg"
   },
   "source": [
    "### Task 3.1 : Transisiont, Emission metrices (20 Points)\n",
    "\n",
    "In this task we are expected to build a function which takes training_corpus as input and return transition counts, emission counts and tag counts. <br> \n",
    "\n",
    "\n",
    "\n",
    "1. `Tranition count`: maps prev_tag, tag) to the number of times it has appeared.\n",
    "2. `Emission_counts`: maps (tag, word) to the number of times it appeared.\n",
    "3. `Tag_counts`: maps (tag) to the number of times it has occured.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gr8yMvKmOsZP"
   },
   "outputs": [],
   "source": [
    "def create_dictionaries(training_corpus, vocab):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        training_corpus: a corpus where each line has a word followed by its tag.\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output: \n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
    "        tag_counts: a dictionary where the keys are the tags and the values are the counts\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the dictionaries using defaultdict\n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    \n",
    "    # Initialize \"prev_tag\" (previous tag) with the start state, denoted by '--s--'\n",
    "    prev_tag = '--s--' \n",
    "    \n",
    "    # use 'i' to track the line number in the corpus\n",
    "    i = 0 \n",
    "    \n",
    "    # Each item in the training corpus contains a word and its POS tag\n",
    "    # Go through each word and its tag in the training corpus\n",
    "    for word_tag in training_corpus:\n",
    "        \n",
    "        # Increment the word_tag count\n",
    "        i += 1\n",
    "            \n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        # get the word and tag using the get_word_tag helper function\n",
    "        word, tag = get_word_tag(word_tag, vocab)        \n",
    "        \n",
    "        # Increment the transition count for the previous word and tag\n",
    "        transition_counts[(prev_tag, tag)] += 1      \n",
    "            \n",
    "        # Increment the emission count for the tag and word\n",
    "        emission_counts[(tag, word)] += 1      \n",
    "            \n",
    "        # Increment the tag count\n",
    "        tag_counts[tag] += 1        \n",
    "            \n",
    "        # Set the previous tag to this tag (for the next iteration of the loop)\n",
    "        prev_tag = tag\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return emission_counts, transition_counts, tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Q-OVNdq6Osoz"
   },
   "outputs": [],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Qu97we4WTse",
    "outputId": "99eb9387-fbf6-4b32-b08c-db06ff3732f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POS tags (number of 'states'): 46\n",
      "View these POS tags (states)\n",
      "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "# get all the POS states. States are parts of speech designation found in the training dataset.\n",
    "states = sorted(tag_counts.keys())\n",
    "print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n",
    "print(\"View these POS tags (states)\")\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vVsk2TqXnq4"
   },
   "source": [
    "##### Expected Output\n",
    "\n",
    "```CPP\n",
    "Number of POS tags (number of 'states'46\n",
    "View these states\n",
    "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94MESMDvWrcK",
    "outputId": "9f0b2588-b881-454f-f2df-b84e2f98762d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition examples: \n",
      "(('--s--', 'IN'), 5050)\n",
      "(('IN', 'DT'), 32364)\n",
      "(('DT', 'NNP'), 9044)\n",
      "\n",
      "emission examples: \n",
      "(('IN', 'In'), 1735)\n",
      "(('DT', 'an'), 3142)\n",
      "(('NNP', 'Oct.'), 317)\n",
      "\n",
      "ambiguous word example: \n",
      "('IN', 'In') 1735\n",
      "('DT', 'an') 3142\n",
      "('NNP', 'Oct.') 317\n"
     ]
    }
   ],
   "source": [
    "print(\"transition examples: \")\n",
    "for ex in list(transition_counts.items())[:3]:\n",
    "    print(ex)\n",
    "print()\n",
    "\n",
    "print(\"emission examples: \")\n",
    "for ex in list(emission_counts.items())[:3]:\n",
    "    print (ex)\n",
    "print()\n",
    "\n",
    "print(\"ambiguous word example: \")\n",
    "for tup,cnt in list(emission_counts.items())[:3]:\n",
    "    print(tup, cnt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGl4GGZ9Xx54"
   },
   "source": [
    "\n",
    "##### Expected Output\n",
    "\n",
    "```CPP\n",
    "transition examples: \n",
    "(('--s--', 'IN'), 5050)\n",
    "(('IN', 'DT'), 32364)\n",
    "(('DT', 'NNP'), 9044)\n",
    "\n",
    "emission examples: \n",
    "(('IN', 'In'), 1735)\n",
    "(('DT', 'an'), 3142)\n",
    "(('NNP', 'Oct.'), 317)\n",
    "\n",
    "ambiguous word example: \n",
    "('IN', 'In') 1735\n",
    "('DT', 'an') 3142\n",
    "('NNP', 'Oct.') 317\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsoeGOxvXq__"
   },
   "source": [
    "### Task 3.2: Predict (20 Points)\n",
    "\n",
    "You need to complete the `predict_pos` function below which takes preprocessed test corpus (prep), Original tagged test corpus `y`, emission counts, vocab and states. <br>\n",
    "\n",
    "Ultimately in this function for a given preprocessed test corpus, you will assign a parts-of-speech tag to every word in that corpus. Using the original tagged test corpus, you will then compute what percent of the tags you got correct. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pT6XFFHdZaKM"
   },
   "outputs": [],
   "source": [
    "def predict_pos(prep, y, emission_counts, vocab, states):\n",
    "    '''\n",
    "    Input: \n",
    "        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n",
    "        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n",
    "        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "        states: a sorted list of all possible tags for this assignment\n",
    "    Output: \n",
    "        accuracy: Number of times you classified a word correctly\n",
    "    '''\n",
    "    \n",
    "    # Initialize the number of correct predictions to zero\n",
    "    # Get the (tag, word) tuples, stored as a set\n",
    "    num_correct = 0\n",
    "    cnt_all = len(y)\n",
    "\n",
    "    # Get the (tag, word) tuples, stored as a set\n",
    "\n",
    "    for word, line in zip(prep, y):\n",
    "        word_pos = line.split() # Split into a list\n",
    "      \n",
    "      # Go to next word\n",
    "        if len(word_pos)==2:\n",
    "            check = word_pos[1]\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        largest_count = 0\n",
    "        final_tag = ''\n",
    "    \n",
    "        \n",
    "        if word in vocab:\n",
    "            for tag in states:\n",
    "                key = (tag,word)\n",
    "                \n",
    "                if key in emission_counts:\n",
    "                    count = emission_counts[key]\n",
    "                    \n",
    "                    if count > largest_count:\n",
    "                        largest_count = count\n",
    "                        final_tag = tag\n",
    "                        \n",
    "        if final_tag == check:\n",
    "            num_correct += 1\n",
    "\n",
    "    num = num_correct\n",
    "    den = cnt_all \n",
    "   \n",
    "       \n",
    "    ### END CODE HERE ###\n",
    "    accuracy = num / den\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9p6rFQmZhVd",
    "outputId": "c7ac5adf-4cf3-40c8-fb46-5a81ed9213e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction using predict_pos is 0.8889\n"
     ]
    }
   ],
   "source": [
    "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n",
    "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouVVpP11YsyA"
   },
   "source": [
    "##### Expected Output\n",
    "\n",
    "```CPP\n",
    "Accuracy of prediction using predict_pos is 0.8889\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTJ_W6jWb7gc"
   },
   "source": [
    "### Task 3.3 Building Hidden Markov Models for POS. (20 Points)\n",
    "\n",
    "**Hidden Markov Models** (HMMs) are a class of probabilistic graphical model that allow us to predict a sequence of unknown (hidden) variables from a set of observed variables. <br>\n",
    "\n",
    "The Markov Model contains a number of states and the probability of transition between those states. \n",
    "- In this case, the states are the parts-of-speech. \n",
    "- A Markov Model utilizes a transition matrix, `A`. \n",
    "- A Hidden Markov Model adds an observation or emission matrix `B` which describes the probability of a visible observation when we are in a particular state. \n",
    "- In this case, the emissions are the words in the corpus\n",
    "- The state, which is hidden, is the POS tag of that word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJa3ynC-ePOl"
   },
   "source": [
    "### Creating the 'A' transition probabilities matrix\n",
    "\n",
    "We will be using Smoothing to compute the matrix. \n",
    "\n",
    "The smoothing was done as follows: \n",
    "\n",
    "$$ P(t_i | t_{i-1}) = \\frac{C(t_{i-1}, t_{i}) + \\alpha }{C(t_{i-1}) +\\alpha * N}$$\n",
    "\n",
    "- $N$ is the total number of tags\n",
    "- $C(t_{i-1}, t_{i})$ is the count of the tuple (previous POS, current POS) in `transition_counts` dictionary.\n",
    "- $C(t_{i-1})$ is the count of the previous POS in the `tag_counts` dictionary.\n",
    "- $\\alpha$ is a smoothing parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lz5xHj8QegXt"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_transition_matrix\n",
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    ''' \n",
    "    Input: \n",
    "        alpha: number used for smoothing\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        transition_counts: transition count for the previous word and tag\n",
    "    Output:\n",
    "        A: matrix of dimension (num_tags,num_tags)\n",
    "    '''\n",
    "    \n",
    "    num_tags = len(tag_counts)\n",
    "\n",
    "    A = np.zeros((num_tags, num_tags))\n",
    "    # Create sorted version of the tag's list\n",
    "    sorted_tags = sorted(states)\n",
    "    \n",
    "\n",
    "    ### Your Code Goes Here ###\n",
    "    for i in range(num_tags):\n",
    "        for j in range(num_tags):\n",
    "            previous_tag = sorted_tags[i]\n",
    "            tag = sorted_tags[j]\n",
    "       \n",
    "         \n",
    "         # calculate the smoothed probability\n",
    "            A[i][j] = (transition_counts[(previous_tag, tag)] + alpha)/(tag_counts[previous_tag] + alpha*num_tags)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7qelj9meq2b",
    "outputId": "859f6e01-15ff-4d0a-837e-56efde1fb68d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A at row 0, col 0: 0.007047013\n",
      "A at row 3, col 1: 0.0000\n",
      "View a subset of transition matrix A\n",
      "          CD        DT            EX        FW        IN\n",
      "CD  0.201542  0.028850  2.734628e-08  0.000055  0.089997\n",
      "DT  0.022922  0.001576  1.221866e-08  0.000257  0.009665\n",
      "EX  0.000001  0.002319  1.158687e-06  0.000001  0.000001\n",
      "FW  0.000004  0.008550  4.272664e-06  0.239273  0.029913\n",
      "IN  0.059328  0.328388  1.582898e-03  0.000203  0.020415\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "# Testing your function\n",
    "print(f\"A at row 0, col 0: {A[0,5]:.9f}\")\n",
    "print(f\"A at row 3, col 1: {A[3,6]:.4f}\")\n",
    "\n",
    "print(\"View a subset of transition matrix A\")\n",
    "A_sub = pd.DataFrame(A[10:15,10:15], index=states[10:15], columns = states[10:15] )\n",
    "print(A_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgBwL-UaZJgw"
   },
   "source": [
    "#### Expected Output: \n",
    "\n",
    "```CPP\n",
    "A at row 0, col 0: 0.007047013\n",
    "A at row 3, col 1: 0.0000\n",
    "View a subset of transition matrix A\n",
    "          CD        DT            EX        FW        IN\n",
    "CD  0.201542  0.028850  2.734628e-08  0.000055  0.089997\n",
    "DT  0.022922  0.001576  1.221866e-08  0.000257  0.009665\n",
    "EX  0.000001  0.002319  1.158687e-06  0.000001  0.000001\n",
    "FW  0.000004  0.008550  4.272664e-06  0.239273  0.029913\n",
    "IN  0.059328  0.328388  1.582898e-03  0.000203  0.020415\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNm4kE26ftrC"
   },
   "source": [
    "#### Creating 'B' emission probabilities matrix\n",
    "\n",
    "Now you will create the `B` transition matrix which computes the emission probability. \n",
    "\n",
    "You will use smoothing as defined below: \n",
    "\n",
    "$$P(w_i | t_i) = \\frac{C(t_i, word_i)+ \\alpha}{C(t_{i}) +\\alpha * N}\\$$\n",
    "\n",
    "- $C(t_i, word_i)$ is the number of times $word_i$ was associated with $tag_i$ in the training data (stored in `emission_counts` dictionary).\n",
    "- $C(t_i)$ is the number of times $tag_i$ was in the training data (stored in `tag_counts` dictionary).\n",
    "- $N$ is the number of words in the vocabulary\n",
    "- $\\alpha$ is a smoothing parameter. \n",
    "\n",
    "The matrix `B` is of dimension (num_tags, N), where num_tags is the number of possible parts-of-speech tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AuOi6kfugBP_"
   },
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: create_emission_matrix\n",
    "\n",
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        alpha: tuning parameter used in smoothing \n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        B: a matrix of dimension (num_tags, len(vocab))\n",
    "    '''\n",
    "    num_tags = len(tag_counts)\n",
    "    B = np.zeros((num_tags, len(vocab)))\n",
    "    # Create sorted version of the tag's list\n",
    "    tags_sort = sorted(states)\n",
    "\n",
    "    for i in range(num_tags):\n",
    "        for j in range(len(vocab)):\n",
    "            \n",
    "            key = (tags_sort[i], list(vocab)[j])\n",
    "            B[i][j] = (emission_counts[key] + alpha)/(tag_counts[tags_sort[i]] + alpha*len(vocab))   \n",
    "   \n",
    "\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aHk48VkVgJ6y",
    "outputId": "ee5d3005-6701-4192-e6d3-cefcbb51fa2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Matrix position at row 0, column 0: 0.000006032\n",
      "View Matrix position at row 3, column 1: 0.000000720\n",
      "              725      adroitly     engineers      promoted       synergy\n",
      "CD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08\n",
      "NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\n",
      "NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\n",
      "VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\n",
      "RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\n",
      "RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07\n"
     ]
    }
   ],
   "source": [
    "# creating your emission probability matrix. this takes a few minutes to run. \n",
    "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
    "\n",
    "print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n",
    "print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n",
    "\n",
    "# Try viewing emissions for a few words in a sample dataframe\n",
    "cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n",
    "\n",
    "# Get the integer ID for each word\n",
    "cols = [vocab[a] for a in cidx]\n",
    "\n",
    "# Choose POS tags to show in a sample dataframe\n",
    "rvals =['CD','NN','NNS', 'VB','RB','RP']\n",
    "\n",
    "# For each POS tag, get the row number from the 'states' list\n",
    "rows = [states.index(a) for a in rvals]\n",
    "\n",
    "# Get the emissions for the sample of words, and the sample of POS tags\n",
    "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
    "print(B_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P84N8BiukxSb"
   },
   "source": [
    "#### Expected Output: \n",
    "\n",
    "```CPP\n",
    "View Matrix position at row 0, column 0: 0.000006032\n",
    "View Matrix position at row 3, column 1: 0.000000720\n",
    "              725      adroitly     engineers      promoted       synergy\n",
    "CD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08\n",
    "NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\n",
    "NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\n",
    "VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\n",
    "RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\n",
    "RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0sqAdi8sTRM"
   },
   "source": [
    "### Task 3.4 Viterbi Algorithm\n",
    "\n",
    "In this part of the assignment you will implement the Viterbi algorithm. Specifically, you will use your two matrices, `A` and `B` to compute the Viterbi algorithm. \n",
    "\n",
    "We have decomposed this process into three main parts. \n",
    "\n",
    "* **Initialization** - In this part you initialize the `best_paths` and `best_probabilities` matrices that you will be populating in `feed_forward`.\n",
    "* **Feed forward** - At each step, you calculate the probability of each path happening and the best paths up to that point. \n",
    "* **Feed backward**: This allows you to find the best path with the highest probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiPp1B6atGT4"
   },
   "source": [
    "### Task 3.4.1 Initialization \n",
    "Write a program below that initializes the `best_probs` and the `best_paths` matrix. \n",
    "\n",
    "Both matrices will be initialized to zero except for column zero of `best_probs`.  \n",
    "- Column zero of `best_probs` is initialized with the assumption that the first word of the corpus was preceded by a start token (\"--s--\"). \n",
    "- This allows you to reference the **A** matrix for the transition probability\n",
    "\n",
    "Here is how to initialize column 0 of `best_probs`:\n",
    "- The probability of the best path going from the start index to a given POS tag indexed by integer $i$ is denoted by $\\textrm{best_probs}[s_{idx}, i]$.\n",
    "- This is estimated as the probability that the start tag transitions to the POS denoted by index $i$: $\\mathbf{A}[s_{idx}, i]$ AND that the POS tag denoted by $i$ emits the first word of the given corpus, which is $\\mathbf{B}[i, vocab[corpus[0]]]$.\n",
    "- Note that vocab[corpus[0]] refers to the first word of the corpus (the word at position 0 of the corpus). \n",
    "- **vocab** is a dictionary that returns the unique integer that refers to that particular word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HnQNnScHsqUk"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# GRADED FUNCTION: initialize\n",
    "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        states: a list of all possible parts-of-speech\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        A: Transition Matrix of dimension (num_tags, num_tags)\n",
    "        B: Emission Matrix of dimension (num_tags, len(vocab))\n",
    "        corpus: a sequence of words whose POS is to be identified in a list \n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n",
    "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n",
    "    '''\n",
    "    n_tag = len(states)\n",
    "    best_probs = np.zeros((n_tag, len(corpus)))\n",
    "    best_paths = np.zeros((n_tag, len(corpus)))\n",
    "    s_idx = states.index('--s--')\n",
    "    for i in range(n_tag):\n",
    "        if A[s_idx, i] == 0:\n",
    "            best_probs[i, 0] = float('-inf')\n",
    "        else:\n",
    "            best_probs[i, 0] = np.log(A[s_idx, i]) + np.log(B[i, vocab[corpus[0]]])\n",
    "    return best_probs, best_paths\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mXvRxrscu1ub"
   },
   "outputs": [],
   "source": [
    "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gs9rytMIu3Rf",
    "outputId": "66d53e06-da82-43b4-a16a-c86f57c607b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,0]: -22.6098\n",
      "best_paths[2,3]: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\") \n",
    "print(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSAH0VXXlAja"
   },
   "source": [
    "#### Expected Output: \n",
    "\n",
    "```CPP\n",
    "best_probs[0,0]: -22.6098\n",
    "best_paths[2,3]: 0.0000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJgTIbfKvrj7"
   },
   "source": [
    "### Task 3.4.2 Viterbi Forward Implementation (20 Points)\n",
    "\n",
    "\n",
    "In this part of the assignment, you will implement the `viterbi_forward` segment. In other words, you will populate your `best_probs` and `best_paths` matrices.\n",
    "- Walk forward through the corpus.\n",
    "- For each word, compute a probability for each possible tag. \n",
    "\n",
    "Store the best_path and best_prob for every possible tag for each word in the matrices `best_probs` and `best_tags` using the pseudo code below.\n",
    "\n",
    "`for each word in the corpus\n",
    "\n",
    "    for each POS tag type that this word may be\n",
    "    \n",
    "        for POS tag type that the previous word could be\n",
    "        \n",
    "            compute the probability that the previous word had a given POS tag, that the current word has a given POS tag, and that the POS tag would emit this current word.\n",
    "            \n",
    "            retain the highest probability computed for the current word\n",
    "            \n",
    "            set best_probs to this highest probability\n",
    "            \n",
    "            set best_paths to the index 'k', representing the POS tag of the previous word which produced the highest probability `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FJWBl5IYwYS0"
   },
   "outputs": [],
   "source": [
    "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab, corpus):\n",
    "    '''\n",
    "    Input: \n",
    "        A, B: The transiton and emission matrices respectively\n",
    "        test_corpus: a list containing a preprocessed corpus\n",
    "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index \n",
    "    Output: \n",
    "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
    "    '''\n",
    "    n_tag = best_probs.shape[0]\n",
    "    for i in range(1, len(test_corpus)):\n",
    "            \n",
    "        # Write your code here\n",
    "       \n",
    "        for j in range(n_tag):\n",
    "            best_prob_i = float('-inf')\n",
    "            best_path_i = None\n",
    "            for k in range(n_tag):\n",
    "                prob = best_probs[k, i-1] + np.log(A[k,j]) + np.log(B[j, vocab[test_corpus[i]]]) \n",
    "                if prob > best_prob_i:\n",
    "                    best_prob_i = prob\n",
    "                    best_path_i = k\n",
    "            best_probs[j, i] = best_prob_i\n",
    "            best_paths[j, i] = best_path_i\n",
    "                    \n",
    "    return best_probs, best_paths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9e23XCrSwcAo"
   },
   "outputs": [],
   "source": [
    "# this will take a few minutes to run => processes ~ 30,000 words\n",
    "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab, prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByhnCk4UweLT",
    "outputId": "192f4adc-4afa-460b-d952-5ca6cfb05103"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,1]: -24.7822\n",
      "best_probs[0,4]: -49.5601\n"
     ]
    }
   ],
   "source": [
    "# Test this function \n",
    "print(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\") \n",
    "print(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjUDVP7alH0A"
   },
   "source": [
    "#### Expected Output: \n",
    "\n",
    "```CPP\n",
    "best_probs[0,1]: -35.2828\n",
    "best_probs[0,4]: -54.4040\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-xcj0KsxRws"
   },
   "source": [
    "### Task 3.4.3 Viterbi Backward Implementation (20 Points)\n",
    "\n",
    "<a name='2.4'></a>\n",
    "## Part 2.4 Viterbi backward\n",
    "\n",
    "Now you will implement the Viterbi backward algorithm.\n",
    "- The Viterbi backward algorithm gets the predictions of the POS tags for each word in the corpus using the `best_paths` and the `best_probs` matrices.\n",
    "\n",
    "The example below shows how to walk backwards through the best_paths matrix to get the POS tags of each word in the corpus. Recall that this example corpus has three words: \"Loss tracks upward\".\n",
    "\n",
    "POS tag for 'upward' is `RB`\n",
    "- Select the the most likely POS tag for the last word in the corpus, 'upward' in the `best_prob` table.\n",
    "- Look for the row in the column for 'upward' that has the largest probability.\n",
    "- Notice that in row 28 of `best_probs`, the estimated probability is -34.99, which is larger than the other values in the column.  So the most likely POS tag for 'upward' is `RB` an adverb, at row 28 of `best_prob`. \n",
    "- The variable `z` is an array that stores the unique integer ID of the predicted POS tags for each word in the corpus.  In array z, at position 2, store the value 28 to indicate that the word 'upward' (at index 2 in the corpus), most likely has the POS tag associated with unique ID 28 (which is `RB`).\n",
    "- The variable `pred` contains the POS tags in string form.  So `pred` at index 2 stores the string `RB`.\n",
    "\n",
    "\n",
    "POS tag for 'tracks' is `VBZ`\n",
    "- The next step is to go backward one word in the corpus ('tracks').  Since the most likely POS tag for 'upward' is `RB`, which is uniquely identified by integer ID 28, go to the `best_paths` matrix in column 2, row 28.  The value stored in `best_paths`, column 2, row 28 indicates the unique ID of the POS tag of the previous word.  In this case, the value stored here is 40, which is the unique ID for POS tag `VBZ` (verb, 3rd person singular present).\n",
    "- So the previous word at index 1 of the corpus ('tracks'), most likely has the POS tag with unique ID 40, which is `VBZ`.\n",
    "- In array `z`, store the value 40 at position 1, and for array `pred`, store the string `VBZ` to indicate that the word 'tracks' most likely has POS tag `VBZ`.\n",
    "\n",
    "POS tag for 'Loss' is `NN`\n",
    "- In `best_paths` at column 1, the unique ID stored at row 40 is 20.  20 is the unique ID for POS tag `NN`.\n",
    "- In array `z` at position 0, store 20.  In array `pred` at position 0, store `NN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bmmwhv-Ex6VC"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: viterbi_backward\n",
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "    '''\n",
    "    This function returns the best path.\n",
    "    \n",
    "    '''\n",
    "    maxProb = float('-inf')\n",
    "    maxIndex = 0\n",
    "    \n",
    "    #### Your Code Goes Here ####\n",
    "    pred = []\n",
    "\n",
    "    \n",
    "    maxProb = max(list(best_probs[:,-1]))\n",
    "    idx = list(best_probs[:,-1]).index(maxProb)\n",
    "    pred.append(states[idx])\n",
    "\n",
    "   \n",
    "    k_last = int(best_paths[idx, -1])\n",
    "\n",
    "    \n",
    "    for i in reversed(range(len(corpus)-1)):\n",
    "        pred.append(states[k_last])\n",
    "        k_last = int(best_paths[k_last, i])\n",
    "\n",
    "    \n",
    "    pred.reverse()\n",
    "\n",
    "    return pred\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4FMVOc_6zXVT",
    "outputId": "c8c00948-62bd-4f83-b743-00b5cce19ed0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for pred[-7:m-1] is: \n",
      " ['see', 'them', 'here', 'with', 'us', '.'] \n",
      " ['VB', 'PRP', 'RB', 'IN', 'PRP', '.'] \n",
      "\n",
      "The prediction for pred[0:8] is: \n",
      " ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN'] \n",
      " ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken']\n"
     ]
    }
   ],
   "source": [
    "# Run and test your function\n",
    "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
    "m=len(pred)\n",
    "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
    "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JIYGEv6lO_f"
   },
   "source": [
    "#### Expected Output: \n",
    "\n",
    "```CPP\n",
    "The prediction for pred[-7:m-1] is: \n",
    " ['see', 'them', 'here', 'with', 'us', '.'] \n",
    " ['VB', 'PRP', 'RB', 'IN', 'PRP', '.'] \n",
    "\n",
    "The prediction for pred[0:8] is: \n",
    " ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN'] \n",
    " ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQHA_61Qzbn9"
   },
   "source": [
    "### Predicting on the dataset and print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1n_WCDqzax6",
    "outputId": "2e899c55-f669-4e44-8bc1-5c0f13ebd5f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word is: temperature\n",
      "Your prediction is: NN\n",
      "Your corresponding label y is:  temperature\tNN\n",
      "\n",
      "Accuracy of the Viterbi algorithm is 0.9531\n"
     ]
    }
   ],
   "source": [
    "print('The word is:', prep[3])\n",
    "print('Your prediction is:', pred[3])\n",
    "print('Your corresponding label y is: ', y[3])\n",
    "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzROssM3lS-x"
   },
   "source": [
    "#### Expected Output: \n",
    "\n",
    "```CPP\n",
    "The word is: points\n",
    "Your prediction is: VBZ\n",
    "Your corresponding label y is:  points\tNNS\n",
    "\n",
    "Accuracy of the Viterbi algorithm is 0.9531\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# Part 2: LSTMs for POS tagging - using PyTorch (40 Points)\n",
    "In this part, We will be building a bidirectional LSTM network to train and inference POS tagging on UDPOS dataset.<br>\n",
    "\n",
    "PyTorch makes it easy by abstracting most of the details that go in building,training and inferencing a neural network. We recommend going through every PyTorch function that this notebook uses to gain more understanding.   \n",
    "\n",
    "If you need a refresher or have never worked with Neural Networks before, here are a few resources:\n",
    "- https://web.stanford.edu/~jurafsky/slp3/7.pdf\n",
    "- https://web.stanford.edu/~jurafsky/slp3/9.pdf\n",
    "- https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "We will be using PyTorch for defining, training and inferencing a neural network for our POS Tagging problem. If you have not used any deep learning framework/library, we recommend you spend some time understanding how to use these libraries. \n",
    "\n",
    "PyTorch Resources:\n",
    "- https://pytorch.org/tutorials/\n",
    "- https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# a package that provides processing utilities and popular datasets for natural language\n",
    "from torchtext import data\n",
    "from torchtext.datasets import UDPOS\n",
    "from torchtext.data  import Field\n",
    "\n",
    "import spacy\n",
    "from tqdm import tqdm \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Using a seed to maintain consistent and reproducible results\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell downloads and prepares data, a TorchText Dataset Object\n",
    "\n",
    "TEXT = Field(lower = True)\n",
    "UD_TAGS = Field(unk_token = None)\n",
    "fields = ((\"text\", TEXT), (\"udtags\", UD_TAGS))\n",
    "train_data, valid_data, test_data = UDPOS.splits(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the torchtext dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset 12543\n",
      "TEXT  1 al - zaman : american forces killed shaikh abdullah al - ani , the preacher at the mosque in the town of qaim , near the syrian border .\n",
      "TAGS  1 PROPN PUNCT PROPN PUNCT ADJ NOUN VERB PROPN PROPN PROPN PUNCT PROPN PUNCT DET NOUN ADP DET NOUN ADP DET NOUN ADP PROPN PUNCT ADP DET ADJ NOUN PUNCT\n",
      "TEXT  2 [ this killing of a respected cleric will be causing us trouble for years to come . ]\n",
      "TAGS  2 PUNCT DET NOUN ADP DET ADJ NOUN AUX AUX VERB PRON NOUN ADP NOUN PART VERB PUNCT PUNCT\n",
      "TEXT  3 dpa : iraqi authorities announced that they had busted up 3 terrorist cells operating in baghdad .\n",
      "TAGS  3 PROPN PUNCT ADJ NOUN VERB SCONJ PRON AUX VERB ADP NUM ADJ NOUN VERB ADP PROPN PUNCT\n",
      "TEXT  4 two of them were being run by 2 officials of the ministry of the interior !\n",
      "TAGS  4 NUM ADP PRON AUX AUX VERB ADP NUM NOUN ADP DET PROPN ADP DET PROPN PUNCT\n",
      "TEXT  5 the moi in iraq is equivalent to the us fbi , so this would be like having j. edgar hoover unwittingly employ at a high level members of the weathermen bombers back in the 1960s .\n",
      "TAGS  5 DET PROPN ADP PROPN AUX ADJ ADP DET PROPN PROPN PUNCT ADV PRON AUX VERB SCONJ VERB PROPN PROPN PROPN ADV VERB ADP DET ADJ NOUN NOUN ADP DET PROPN NOUN ADV ADP DET NOUN PUNCT\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the dataset\", len(train_data))\n",
    "for i in range(0,5):\n",
    "    print(\"TEXT \", i+1 ,*(train_data[i].__dict__['text']))\n",
    "    print(\"TAGS \", i+1 ,*(train_data[i].__dict__['udtags']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Vector initialization \n",
    "Vectorizing the input words is an impotant step in the NLP pipeline that can determine the end performance of neural networks. GloVe vectors capture both global statistics and local statistics of a corpus. We use GloVe to convert words to embeddings in the vector space based on their semantics. \n",
    "\n",
    "To learn more about GloVe please read the following resource:\n",
    "- https://nlp.stanford.edu/pubs/glove.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the words should have atleast a min frequency of 2 to build its vocab\n",
    "MIN_FREQ = 2\n",
    "\n",
    "# Torch text builds the vocabulary based on word representations from glove. \n",
    "TEXT.build_vocab(train_data, \n",
    "                 min_freq = MIN_FREQ,\n",
    "                 vectors = \"glove.6B.100d\",\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "\n",
    "UD_TAGS.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tags in the dataset\n",
    "len(UD_TAGS.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expected output\n",
    "18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "# Part 2.1: Building the neural network\n",
    "\n",
    "We will make use of the GloVe embeddings and build a bi-directional LSTM. You will be able to tune the hyper parameters of the network and see what works. \n",
    "\n",
    "It involves duplicating the first recurrent layer in the network so that there are now two layers side-by-side, then providing the input sequence as-is as input to the first layer and providing a reversed copy of the input sequence to the second.\n",
    "\n",
    "The idea is to split the state neurons of a regular RNN in a part that is responsible for the positive time direction (forward states) and a part for the negative time direction (backward states)\n",
    "\n",
    "More on it here: https://maxwell.ict.griffith.edu.au/spl/publications/papers/ieeesp97_schuster.pdf\n",
    "\n",
    "All the internal computations/details will be taken care by PyTorch. You will be able to implement many variations of this neural networks with minor changes in code. Expect your neural network definition to be under 10 lines.\n",
    "\n",
    "Your PyTorch model (inherits torch.nn.Module) definition contains defining two functions:\n",
    "    -Init : Which specifies what layers to initialize.\n",
    "    -Forward: Which defines the order of computations in these layers. <br>\n",
    "**Note** - We will not grade based on accuracy, We grade if your model converges. You can follow your order of code, if you think the comments are not helping.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1.2 Building LSTM network - 20 Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPOSTagger(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim, \n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout, \n",
    "                 pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define an embedding layer that converts the words to embeddings based on GloVe.\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # Define a bi-directional LSTM layer with the hyperparameters.\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            bidirectional=bidirectional, \n",
    "                            dropout=dropout if n_layers > 1 else 0)\n",
    "        \n",
    "        # Define a dropout layer that helps in regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Define a Linear layer which can associate lstm output to the final output \n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # pass text through embedding layer\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        # pass embeddings into LSTM\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # pass the LSTM output to dropout and fully connected linear layer\n",
    "        output = self.dropout(output)\n",
    "        linear_input = output.view(output.shape[0]*output.shape[1], output.shape[2])\n",
    "        fc_output = self.fc(linear_input)\n",
    "        \n",
    "        # we use our outputs to make a prediction of what the tag should be\n",
    "        predictions = fc_output.view(output.shape[0], output.shape[1], fc_output.shape[1])\n",
    "        \n",
    "        # predictions = [sent len, batch size, output dim]\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEXT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\UNI's\\Applied\\NEU\\Data Analytics Engg\\NLP\\new\\NLP-Assignment-4\\new.ipynb Cell 57\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y111sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Tweak the Nones\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y111sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m INPUT_DIM \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(TEXT\u001b[39m.\u001b[39mvocab)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y111sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m EMBEDDING_DIM \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y111sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m HIDDEN_DIM \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TEXT' is not defined"
     ]
    }
   ],
   "source": [
    "# Tweak the Nones\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = len(UD_TAGS.vocab)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = LSTMPOSTagger(INPUT_DIM, \n",
    "                        EMBEDDING_DIM, \n",
    "                        HIDDEN_DIM, \n",
    "                        OUTPUT_DIM, \n",
    "                        N_LAYERS, \n",
    "                        BIDIRECTIONAL, \n",
    "                        DROPOUT, \n",
    "                        PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\UNI's\\Applied\\NEU\\Data Analytics Engg\\NLP\\new\\NLP-Assignment-4\\new.ipynb Cell 58\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y112sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m m\u001b[39m.\u001b[39mnamed_parameters():\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y112sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         nn\u001b[39m.\u001b[39minit\u001b[39m.\u001b[39mnormal_(param\u001b[39m.\u001b[39mdata, std\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y112sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39mapply(init_weights)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y112sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# initializing model embeddings with glove word vectors\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y112sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pretrained_embeddings \u001b[39m=\u001b[39m TEXT\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mvectors\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# initializing model weights for better convergence\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, std=0.1)\n",
    "model.apply(init_weights)\n",
    "\n",
    "# initializing model embeddings with glove word vectors\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# making the padding embeddings as all zero, as we don't want to learn paddings.\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your PC doesn't have enough CPU Ram or Video memory, try decreasing the batch_size\n",
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BucketIterator allows for data to be split into buckets of equal size,\n",
    "# any remaining space is filled with pad token\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)\n",
    "TAG_PAD_IDX = UD_TAGS.vocab.stoi[UD_TAGS.pad_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss Function\n",
    "Optimizers are algorithms or methods used to change the attributes of the neural network such as weights and learning rate to reduce the losses. Optimizers are used to solve optimization problems by minimizing the function.\n",
    "- PyTorch provides many Optimizer Algorithms, feel free to try them and the one that works best for you. \n",
    "- Link - https://pytorch.org/docs/stable/optim.html\n",
    "- We will be using CrossEntropyLoss as predicting a word tag is a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer to train the model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# ignoring the padding in our loss calculation\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# use gpu if available, These lines move your model to gpu from cpu if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# If this line prints cuda, your machine is equipped with a Nvidia GPU and PyTorch is utilizing the GPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to check for accurcy of the model ignoring the pad index\n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    non_pad_elements = (y != TAG_PAD_IDX).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / y[non_pad_elements].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1.2 - Training and Testing LSTM - 20 Points\n",
    "- Decide your epochs to train based on loss and accuracy\n",
    "- Fill single line PyTorch commands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\UNI's\\Applied\\NEU\\Data Analytics Engg\\NLP\\new\\NLP-Assignment-4\\new.ipynb Cell 66\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y123sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     train_acc \u001b[39m=\u001b[39m categorical_accuracy(predictions, tags)  \u001b[39m# calculate accuracy\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y123sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     train_epoch_acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_acc\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y123sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# calculate gradients\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y123sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# update weights\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y123sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m train_epoch_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_iterator)\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\chodisetti_mohit\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\chodisetti_mohit\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    model.train()\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    print(f'Epoch: {epoch+1:02}\\n')\n",
    "    for batch in train_iterator:\n",
    "        \n",
    "        # returns a batch of text to train on (sent len, batch size)\n",
    "        text = batch.text\n",
    "        tags = batch.udtags\n",
    "        \n",
    "        optimizer.zero_grad()  # reset gradients\n",
    "        \n",
    "        predictions = model(text)  # feed batch to model\n",
    "        \n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        \n",
    "        loss = criterion(predictions, tags)  # calculate loss\n",
    "        \n",
    "        train_epoch_loss += loss.item()\n",
    "        train_acc = categorical_accuracy(predictions, tags)  # calculate accuracy\n",
    "        train_epoch_acc += train_acc.item()\n",
    "        \n",
    "        loss.backward()  # calculate gradients\n",
    "        \n",
    "        optimizer.step()  # update weights\n",
    "        \n",
    "    train_epoch_loss /= len(train_iterator)\n",
    "    train_epoch_acc /= len(train_iterator)\n",
    "    \n",
    "    print(f'\\t [Train Loss] : {train_epoch_loss:.3f} | [Train Acc] : {train_epoch_acc*100:.2f}%\\n')\n",
    "    \n",
    "    val_epoch_loss = 0\n",
    "    val_epoch_acc = 0\n",
    "    \n",
    "    model.eval()  # move to validation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in valid_iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.udtags\n",
    "        \n",
    "            predictions = model(text)  # feed batch to model\n",
    "            \n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)  # calculate loss\n",
    "            \n",
    "            val_epoch_loss += loss.item()\n",
    "            val_acc = categorical_accuracy(predictions, tags)  # calculate accuracy\n",
    "            val_epoch_acc += val_acc.item()\n",
    "            \n",
    "        val_epoch_loss /= len(valid_iterator)\n",
    "        val_epoch_acc /= len(valid_iterator)\n",
    "        print(f'\\t [Val Loss] : {val_epoch_loss:.3f} | [Val Acc] : {val_epoch_acc*100:.2f}%\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\n",
      "\t [Train Loss] : 0.159 | [Train Acc] : 94.82%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 1/10 [00:50<07:31, 50.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Val Loss] : 0.348 | [Val Acc] : 89.33%\n",
      "\n",
      "Epoch: 02\n",
      "\n",
      "\t [Train Loss] : 0.151 | [Train Acc] : 95.05%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 2/10 [01:37<06:26, 48.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Val Loss] : 0.347 | [Val Acc] : 89.11%\n",
      "\n",
      "Epoch: 03\n",
      "\n",
      "\t [Train Loss] : 0.142 | [Train Acc] : 95.36%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 3/10 [02:22<05:28, 47.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Val Loss] : 0.352 | [Val Acc] : 88.75%\n",
      "\n",
      "Epoch: 04\n",
      "\n",
      "\t [Train Loss] : 0.135 | [Train Acc] : 95.60%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 4/10 [03:07<04:35, 45.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Val Loss] : 0.344 | [Val Acc] : 89.51%\n",
      "\n",
      "Epoch: 05\n",
      "\n",
      "\t [Train Loss] : 0.128 | [Train Acc] : 95.81%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 5/10 [03:51<03:48, 45.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Val Loss] : 0.343 | [Val Acc] : 89.50%\n",
      "\n",
      "Epoch: 06\n",
      "\n",
      "\t [Train Loss] : 0.123 | [Train Acc] : 95.90%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 6/10 [04:37<03:01, 45.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Val Loss] : 0.336 | [Val Acc] : 89.77%\n",
      "\n",
      "Epoch: 07\n",
      "\n",
      "\t [Train Loss] : 0.119 | [Train Acc] : 96.13%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 7/10 [05:21<02:15, 45.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Val Loss] : 0.333 | [Val Acc] : 89.83%\n",
      "\n",
      "Epoch: 08\n",
      "\n",
      "\t [Train Loss] : 0.112 | [Train Acc] : 96.33%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 8/10 [06:07<01:30, 45.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Val Loss] : 0.337 | [Val Acc] : 89.78%\n",
      "\n",
      "Epoch: 09\n",
      "\n",
      "\t [Train Loss] : 0.109 | [Train Acc] : 96.36%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 9/10 [06:52<00:45, 45.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Val Loss] : 0.345 | [Val Acc] : 89.85%\n",
      "\n",
      "Epoch: 10\n",
      "\n",
      "\t [Train Loss] : 0.102 | [Train Acc] : 96.67%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [07:37<00:00, 45.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Val Loss] : 0.339 | [Val Acc] : 89.81%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Batch' object has no attribute 'ud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mf:\\UNI's\\Applied\\NEU\\Data Analytics Engg\\NLP\\new\\NLP-Assignment-4\\new.ipynb Cell 68\u001b[0m in \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y112sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m test_iterator:\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y112sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     text \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mtext\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/UNI%27s/Applied/NEU/Data%20Analytics%20Engg/NLP/new/NLP-Assignment-4/new.ipynb#Y112sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m     tags \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39;49mud\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'ud'"
     ]
    }
   ],
   "source": [
    "# testing the accuracy on test set\n",
    "test_acc=0\n",
    "model.eval()\n",
    "\n",
    "# Computes without the gradients. Use this while testing your model.\n",
    "# As we do not intend to learn from the data\n",
    "with torch.no_grad():\n",
    "    for batch in test_iterator:\n",
    "        text = batch.text\n",
    "        tags = batch.udtags\n",
    "\n",
    "        # Add the same command that feeds the batch to the model\n",
    "        predictions = model(text)\n",
    "\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        # Make use of the categorical accuracy function and calculate accuracy\n",
    "        batch_acc = categorical_accuracy(predictions, tags)\n",
    "        \n",
    "        # Accumulate batch accuracy to total test accuracy\n",
    "        test_acc += batch_acc.item()\n",
    "\n",
    "# Calculate overall test accuracy\n",
    "final_acc = test_acc / len(test_iterator)\n",
    "\n",
    "print(f'Test Acc: {final_acc*100:.2f}%\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***: You are using a different dataset compared to part 1 and 2. This part of the assignment is designed/aimed to help develop basic understanding of Neural Networks. Although we expect an accuracy of above 85%, we do not grade based on the accuracy output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different inputs to these function.\n",
    "def test_lstm(test_sentence):\n",
    "    x= test_sentence.unsqueeze(-1).to(device)\n",
    "    pred = model(x)\n",
    "    pred = pred.argmax(-1)\n",
    "    pred_tags = [UD_TAGS.vocab.itos[t.item()] for t in pred]\n",
    "    true_tags = [UD_TAGS.vocab.itos[t.item()] for t in test_labels]\n",
    "    tokenized_sentence = [TEXT.vocab.itos[t.item()] for t in test_sentence]\n",
    "    return tokenized_sentence, true_tags,pred_tags    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['expensive', 'yep', '<unk>', '<unk>', '<unk>', 'm', 'shrimp', 'crab', '<unk>', '=)', 'green', 'spanish', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '--', '--', '--', '<unk>', '<unk>', '--', '<unk>', '<unk>', '**', '--', 'cindy', '<unk>', 'reply', '..', '<unk>', '<unk>', '<unk>', '--', '**', '**', '*', '<unk>', 'regards', '<unk>', '<unk>', '<unk>', 'thanks', '<unk>', '<unk>', 'thanks', '<unk>', 'thanks', '<unk>', 'thanks', 'thanks', 'thanks', 'jeff', '<unk>', '<unk>', '<unk>', 'thanks', 'martin', 'vince', 'vince', 'vince', '<unk>', '<unk>', '<unk>', 'vince', 'mike', 'vince', '<unk>', 'vince', '<unk>', 'vince', 'frank', 'thanks', 'thanks', 'mark', 'mark', '?', 'thanks', '<unk>', 'michael', 'thanks', '<unk>', 'fyi', '<unk>', '<unk>', 'ken', '?', '?', '???', 'd', '<unk>', 'thanks', '<unk>', 'd', 'mike', 'sara', 'susan', 'ss', 'sara', 'davis', 'paul', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'jeff', 'stephanie', '<unk>', 'm', 'mike', 'yeah', '...']\n",
      "True tags ['PRON', 'DET', 'PRON', 'PRON', 'INTJ', 'PRON', 'ADV', 'PRON', 'PROPN', 'INTJ', 'ADV', 'PROPN', 'VERB', 'SCONJ', 'PRON', 'DET', 'ADP', 'PUNCT', 'ADV', 'PRON', 'PRON', 'DET', 'PROPN', 'CCONJ', 'X', 'PUNCT', 'SCONJ', 'PUNCT', 'ADP', 'VERB', 'PUNCT', 'NOUN', 'ADV', 'PRON', 'PRON', 'ADP', 'PUNCT', 'ADV', 'PRON', 'PRON', 'PRON', 'PRON', 'CCONJ', 'ADV', 'ADJ', 'SCONJ', 'PRON', 'SCONJ', 'PRON', 'ADP', 'ADP', 'PRON', 'PRON', 'DET', 'SCONJ', 'VERB', 'PRON', 'SCONJ', 'ADP', 'PRON', 'NOUN', 'PRON', 'DET', 'PROPN', 'PRON', 'PROPN', 'SCONJ', 'ADP', 'DET', 'DET', 'PUNCT', 'PRON', 'ADV', 'PRON', 'PRON', 'ADV', 'VERB', 'ADP', 'ADV', 'PRON', 'ADP', 'PROPN', 'AUX', 'NOUN', 'AUX', 'VERB', 'INTJ', 'VERB', 'ADP', 'AUX', 'ADP', 'PUNCT', 'PUNCT', 'VERB', 'ADV', 'PROPN', 'PRON', 'NOUN', 'ADJ', 'VERB', 'DET', 'AUX', 'AUX', 'NOUN', 'PROPN', 'VERB', 'DET', 'PRON', 'PRON', 'PRON', 'ADJ', 'PRON', 'PRON', 'ADP', 'PUNCT', 'VERB', 'AUX', 'NOUN', 'VERB', 'PUNCT', 'VERB', 'AUX', 'VERB', 'ADJ', 'ADP', 'PRON', 'ADJ', 'PROPN']\n",
      "Predicted Tags ['ADJ', 'PUNCT', 'NUM', 'NUM', 'NUM', 'NUM', 'NOUN', 'NOUN', 'NOUN', 'SYM', 'ADJ', 'ADJ', 'NOUN', 'NOUN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'NUM', 'NUM', 'NUM', 'NUM', 'NUM', 'NUM', 'NUM', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PUNCT', 'PUNCT', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PROPN', 'PUNCT', 'PUNCT', 'PROPN', 'PROPN', 'NOUN', 'PUNCT', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'X', 'NOUN', 'X', 'X', 'X', 'NOUN', 'X', 'NUM', 'NOUN', 'NUM', 'NOUN', 'NUM', 'NOUN', 'NOUN', 'NOUN', 'PROPN', 'PROPN', 'NUM', 'NUM', 'NOUN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'NOUN', 'NOUN', 'PROPN', 'PROPN', 'PUNCT', 'NOUN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'ADV', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PUNCT', 'PUNCT', 'X', 'NUM', 'NOUN', 'NOUN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'NUM', 'NUM', 'NUM', 'NUM', 'NUM', 'NUM', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'INTJ', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = text[0]\n",
    "test_labels = tags[0:len(test_sentence)]\n",
    "print(test_lstm(test_sentence)[0])\n",
    "print(\"True tags\", test_lstm(test_sentence)[1])\n",
    "print(\"Predicted Tags\",test_lstm(test_sentence)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model if it performs well. This saves all the trained weights,\n",
    "# so that you don't have to train again in your codewalk\n",
    "torch.save(model.state_dict(), \"./LSTMPOSTAG.pth\")\n",
    "\n",
    "# loading?l\n",
    "# model.load_state_dict(torch.load(\"./LSTMPOSTAG.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory Questions: 10 Points\n",
    "*** Q1. Give some real word examples where POS tagging is used   ***<br>\n",
    "*** Q2. What are the hidden variables in HMM in this assignment? Why are they called hidden? *** <br>\n",
    "*** Q3. How Viterbi Algorithm provides more efficient estimation compared to brute force calculation of all tag combinations? *** <br>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0vVsk2TqXnq4",
    "vGl4GGZ9Xx54",
    "ouVVpP11YsyA",
    "GgBwL-UaZJgw",
    "ZSAH0VXXlAja",
    "JjUDVP7alH0A",
    "9JIYGEv6lO_f",
    "LzROssM3lS-x"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
